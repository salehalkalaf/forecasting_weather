# -*- coding: utf-8 -*-
"""789-checkpoint.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15BI3pLHECNlY3FVymoYOU1s5emFhnAPm
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv("jordan-airports-historical-observations.csv", encoding='ISO-8859-1')
df.head()



unique=df['identifier'].unique()
unique

df.info()

# Assuming your data is in a DataFrame called 'df' and the column is named 'time'
df['time'] = pd.to_datetime(df['time'])

# Now, 'time' column is converted to datetime format

df.info()

df = df.drop_duplicates()

df.info()

uniquelat=df['lat'].unique()
uniquelat

df['lat'].isnull().sum()

uniquelng=df['lng'].unique()
uniquelng

df['lng'].isnull().sum()

unique=df['identifier'].unique()
unique

unique_tem=df['temperature'].unique()
unique_tem

df['temperature'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Replace 'temperature_column_name' with the actual column name containing the 'temperature' values
rows_with_nan_indices = df[df['temperature'].isnull()].index

# Display or further process the row indices with NaN values in the 'temperature' column
print("Row indices with NaN values in the 'temperature' column:")
print(rows_with_nan_indices)



df['temperature'] = df['temperature'].interpolate()

df['temperature'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'temperature' is the column with missing values

# Create a new column to store the original values before interpolation
df['temperature_original'] = df['temperature']

# Use interpolate to fill missing values
df['temperature'] = df['temperature'].interpolate(limit_area='inside', limit_direction='both')

# Indices of interest
indices_of_interest = [343, 345, 1172, 1174, 1507, 2405, 2905, 3269, 3422, 3967, 4138]

# Filter indices that are present in the DataFrame
valid_indices = set(indices_of_interest).intersection(df.index)

# Extract the predicted values for the valid indices
predicted_values = df.loc[valid_indices, 'temperature']

# Display the indices and predicted values
for index, value in predicted_values.iteritems():
    print(f"Index: {index}, Predicted Value: {value}")

df

df.drop('temperature_original',axis=1)

unique_dew=df['dew_point'].unique()
unique_dew

df['dew_point'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Replace 'temperature_column_name' with the actual column name containing the 'temperature' values
rows_with_nan_indices = df[df['dew_point'].isnull()].index

# Display or further process the row indices with NaN values in the 'temperature' column
print("Row indices with NaN values in the 'dew_point' column:")
print(rows_with_nan_indices)

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'dew_point' is the column with missing values
# Assuming 'time' is a column indicating the order of observations (temporal or spatial)

# Sort the DataFrame based on the 'time' column
df = df.sort_values(by='time')

# Create a new column to store the original 'dew_point' values before imputation
df['dew_point_original'] = df['dew_point']

# Use linear interpolation to fill missing values
df['dew_point'] = df['dew_point'].interpolate(method='linear')

# Display or further process the DataFrame with interpolated values
print(df)

df['dew_point'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'dew_point' is the column with missing values
# Assuming 'time' is a column indicating the order of observations (temporal or spatial)

# Create a new column to store the original values before interpolation
df['dew_point_original'] = df['dew_point']

# Use linear interpolation to fill missing values
df['dew_point'] = df['dew_point'].interpolate(method='linear')

# Indices of interest
indices_of_interest = [343, 1172, 1174, 1507, 2405, 2905, 3269, 3422, 3967, 4138]

# Filter indices that are present in the DataFrame
valid_indices = set(indices_of_interest).intersection(df.index)

# Extract the predicted values for the valid indices
predicted_values = df.loc[valid_indices, 'dew_point']

# Display the indices and predicted values
for index, value in predicted_values.iteritems():
    print(f"Index: {index}, Predicted Value: {value}")

df

df['wind_speed'].unique()

df['wind_speed'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'wind_speed' is the column with potential null values

# Drop rows with null values in the 'wind_speed' column
df.dropna(subset=['wind_speed'], inplace=True)

# Now 'df' contains no null values in the 'wind_speed' column

df['wind_speed'].isnull().sum()

df['wind_direction'].unique()

df['wind_direction'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Replace 'temperature_column_name' with the actual column name containing the 'temperature' values
rows_with_nan_indices = df[df['wind_direction'].isnull()].index

# Display or further process the row indices with NaN values in the 'temperature' column
print("Row indices with NaN values in the 'wind_direction' column:")
print(rows_with_nan_indices)

# Assuming your data is in a DataFrame called 'df'
# Assuming 'wind_direction' is the column with missing values

# Use linear interpolation with limitations
df['wind_direction'] = df['wind_direction'].interpolate(limit_area='inside', limit_direction='both')

df['wind_direction'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'wind_direction' is the column with missing values

# Use linear interpolation with limitations
df['wind_direction'] = df['wind_direction'].interpolate(limit_area='inside', limit_direction='both')

# Indices of interest
indices_of_interest = [2885, 7134, 7137, 7138, 7239, 7453, 7454, 7892, 7893, 8029]

# Extract the predicted values for the specified indices
predicted_values = df.loc[indices_of_interest, 'wind_direction']

# Display the indices and predicted values
for index, value in predicted_values.iteritems():
    print(f"Index: {index}, Predicted Value: {value}")

df['visibility'].unique()

df['visibility'].isnull().sum()

import pandas as pd

# Assuming your data is in a DataFrame called 'df'
# Assuming 'visibility' is the column for which you want to drop null values

# Drop rows with null values in the 'visibility' column
df.dropna(subset=['visibility'], inplace=True)

# Now 'df' contains no null values in the 'visibility' column

df['visibility'].isnull().sum()

df

## feature engineering

df['day'] = df['time'].dt.day
df['month'] = df['time'].dt.month
df['year'] = df['time'].dt.year
df['hour'] = df['time'].dt.hour

df['temperature_mean'] = df['temperature'].rolling(window=24).mean()  # Example rolling mean over 24 hours

import matplotlib.pyplot as plt

# Histogram for temperature distribution
plt.hist(df['temperature'], bins=20, color='skyblue')
plt.xlabel('Temperature')
plt.ylabel('Frequency')
plt.title('Temperature Distribution')
plt.show()

import seaborn as sns

# Scatter plot between temperature and dew_point
sns.scatterplot(x='temperature', y='dew_point', data=df)
plt.xlabel('Temperature')
plt.ylabel('Dew Point')
plt.title('Scatter Plot: Temperature vs Dew Point')
plt.show()

# Correlation matrix
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Plot time series data
plt.plot(df['time'], df['temperature'], marker='o')
plt.xlabel('Time')
plt.ylabel('Temperature')
plt.title('Temperature Time Series')
plt.xticks(rotation=45)
plt.show()

# Calculate the IQR for the 'temperature' column
import numpy as np

Q1_temp = df['temperature'].quantile(0.25)
Q3_temp = df['temperature'].quantile(0.75)
IQR_temp = Q3_temp - Q1_temp

# Define the outlier threshold for 'temperature'
outlier_threshold_temp = 1.5

# Identify outliers using the IQR method for 'temperature'
outliers_temp = (df['temperature'] < (Q1_temp - outlier_threshold_temp * IQR_temp)) | (df['temperature'] > (Q3_temp + outlier_threshold_temp * IQR_temp))

# Replace outliers with NaN values
df.loc[outliers_temp, 'temperature'] = np.nan

# Alternatively, you can drop rows with outliers
# df_cleaned = df[~outliers_temp]

# Count the number of outliers dropped or replaced
num_outliers_temp = outliers_temp.sum()
print(f"Number of outliers in 'temperature' column: {num_outliers_temp}")

df

import pandas as pd

# Assuming your DataFrame is named 'df'
# Replace 'df' with the actual name of your DataFrame if different

# List of columns to check for null values
columns_to_check = ['lat', 'lng', 'identifier', 'temperature', 'dew_point', 'wind_speed',
                    'wind_direction', 'visibility', 'temperature_original', 'dew_point_original',
                    'day', 'month', 'year', 'hour', 'temperature_mean']

# Calculate the count of null values for each column in the list
null_counts = df[columns_to_check].isnull().sum()

# Print the count of null values in each specified column
print("Null Value Counts:")
print(null_counts)

# Drop rows with null values in the 'temperature_mean' column
df.dropna(subset=['temperature_mean'], inplace=True)

# Print the updated DataFrame to verify
print(df)

from statsmodels.tsa.stattools import adfuller

def check_stationarity(column_name):
    result = adfuller(df[column_name])
    print(f'{column_name} ADF Statistic:', result[0])
    print(f'{column_name} p-value:', result[1])
    print(f'{column_name} Critical Values:', result[4])

# Check stationarity for each variable
check_stationarity('temperature')
# Add other variables as needed

import pandas as pd
from statsmodels.tsa.stattools import adfuller

# Load the data
df = pd.read_csv("jordan-airports-historical-observations.csv")

# Convert 'time' column to datetime format if it exists
if 'time' in df.columns:
    df['time'] = pd.to_datetime(df['time'])
    df.set_index('time', inplace=True)

# Function to check stationarity
def check_stationarity(series, column_name):
    result = adfuller(series.dropna())
    print(f'{column_name} ADF Statistic: {result[0]}')
    print(f'{column_name} p-value: {result[1]}')
    print(f'{column_name} Critical Values: {result[4]}')
    if result[1] < 0.05:
        print(f'{column_name} is stationary.\n')
    else:
        print(f'{column_name} is not stationary.\n')

# List of columns to check
columns_to_check = ['temperature', 'dew_point', 'wind_speed', 'wind_direction', 'visibility']

# Check stationarity for each specified column
for column in columns_to_check:
    if column in df.columns:
        check_stationarity(df[column], column)
    else:
        print(f"Column {column} does not exist in the DataFrame.\n")

df



import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the SARIMA model
model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))
sarima_model = model.fit(disp=False)

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_start = pd.to_datetime('2024-01-07')  # First Sunday of 2024
forecast_end = pd.to_datetime('2024-12-29')  # Last Sunday of 2024
forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
forecast = sarima_model.get_forecast(steps=forecast_steps)
forecast_values = forecast.predicted_mean
forecast_values.index = forecast_index
forecast_conf_int = forecast.conf_int()
forecast_conf_int.index = forecast_index

# Evaluation for 2023
forecast_2023 = sarima_model.get_forecast(steps=len(test))
forecast_values_2023 = forecast_2023.predicted_mean
mae = mean_absolute_error(test, forecast_values_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_values_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Print the first few forecasted values for 2024
forecast_df = pd.DataFrame({'Forecasted Temperature': forecast_values,
                            'Lower Confidence Interval': forecast_conf_int.iloc[:, 0],
                            'Upper Confidence Interval': forecast_conf_int.iloc[:, 1]})
print(forecast_df.head(10))

# Save the forecasted values and confidence intervals for 2024 to a CSV file
forecast_df.to_csv('forecasted_temperature_2024.csv')

import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the SARIMA model
model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))
sarima_model = model.fit(disp=False)

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_start = pd.to_datetime('2024-01-07')  # First Sunday of 2024
forecast_end = pd.to_datetime('2024-12-29')  # Last Sunday of 2024
forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
forecast = sarima_model.get_forecast(steps=forecast_steps)
forecast_values = forecast.predicted_mean
forecast_values.index = forecast_index
forecast_conf_int = forecast.conf_int()
forecast_conf_int.index = forecast_index

# Evaluation for 2023
forecast_2023 = sarima_model.get_forecast(steps=len(test))
forecast_values_2023 = forecast_2023.predicted_mean
mae = mean_absolute_error(test, forecast_values_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_values_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(train, label='Train')
plt.plot(test, label='Test')
plt.plot(forecast_values_2023, label='Forecast 2023', color='orange')
plt.plot(forecast_values, label='Forecast 2024', color='green')
plt.fill_between(forecast_conf_int.index,
                 forecast_conf_int.iloc[:, 0],
                 forecast_conf_int.iloc[:, 1], color='k', alpha=.2)
plt.title('Weekly Temperature Forecast')
plt.xlabel('Date')
plt.ylabel('Temperature')
plt.legend()
plt.show()

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the ARIMA model
model = ARIMA(train, order=(5, 1, 0))  # Adjust the order (p, d, q) as needed
arima_model = model.fit()

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_start = pd.to_datetime('2024-01-07')  # First Sunday of 2024
forecast_end = pd.to_datetime('2024-12-29')  # Last Sunday of 2024
forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
forecast = arima_model.get_forecast(steps=forecast_steps)
forecast_values = forecast.predicted_mean
forecast_values.index = forecast_index
forecast_conf_int = forecast.conf_int()
forecast_conf_int.index = forecast_index

# Evaluation for 2023
forecast_2023 = arima_model.get_forecast(steps=len(test))
forecast_values_2023 = forecast_2023.predicted_mean
mae = mean_absolute_error(test, forecast_values_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_values_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Print the first few forecasted values for 2024
forecast_df = pd.DataFrame({'Forecasted Temperature': forecast_values,
                            'Lower Confidence Interval': forecast_conf_int.iloc[:, 0],
                            'Upper Confidence Interval': forecast_conf_int.iloc[:, 1]})
print(forecast_df.head(10))

# Save the forecasted values and confidence intervals for 2024 to a CSV file
forecast_df.to_csv('forecasted_temperature_2024.csv')

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the ARIMA model
model = ARIMA(train, order=(5, 1, 0), seasonal_order=(1, 1, 1, 52))  # Adjust the order (p, d, q) and seasonal_order (P, D, Q, S) as needed
arima_model = model.fit()

# Forecasting for the test period
forecast_2023 = arima_model.get_forecast(steps=len(test))
forecast_values_2023 = forecast_2023.predicted_mean
forecast_conf_int_2023 = forecast_2023.conf_int()

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_2024 = arima_model.get_forecast(steps=forecast_steps)
forecast_values_2024 = forecast_2024.predicted_mean
forecast_conf_int_2024 = forecast_2024.conf_int()

# Create a date range for the 2024 forecast
forecast_index_2024 = pd.date_range(start='2024-01-07', periods=forecast_steps, freq='W')
forecast_values_2024.index = forecast_index_2024
forecast_conf_int_2024.index = forecast_index_2024

# Evaluation metrics for 2023 forecast
mae = mean_absolute_error(test, forecast_values_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_values_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Print the first few forecasted values for 2024
forecast_df = pd.DataFrame({
    'Forecasted Temperature': forecast_values_2024,
    'Lower Confidence Interval': forecast_conf_int_2024.iloc[:, 0],
    'Upper Confidence Interval': forecast_conf_int_2024.iloc[:, 1]
})
print(forecast_df.head(10))

# Save the forecasted values and confidence intervals for 2024 to a CSV file
forecast_df.to_csv('forecasted_temperature_2024.csv')

# Plot the results with accurate seasonal forecasts
plt.figure(figsize=(14, 7))
plt.plot(train, label='Train', color='blue', marker='o')
plt.plot(test, label='Test', color='orange', marker='x')
plt.plot(forecast_values_2023, label='Forecast 2023', color='red', linestyle='--', marker='d')
plt.plot(forecast_values_2024, label='Forecast 2024', color='green', linestyle='-', marker='s')
plt.fill_between(forecast_conf_int_2023.index,
                 forecast_conf_int_2023.iloc[:, 0],
                 forecast_conf_int_2023.iloc[:, 1], color='grey', alpha=.3, label='Confidence Interval 2023')
plt.fill_between(forecast_conf_int_2024.index,
                 forecast_conf_int_2024.iloc[:, 0],
                 forecast_conf_int_2024.iloc[:, 1], color='grey', alpha=.3, label='Confidence Interval 2024')
plt.title('Weekly Temperature Forecast with ARIMA Model')
plt.xlabel('Date')
plt.ylabel('Temperature')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the Exponential Smoothing (Holt-Winters) model
model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=52)
hw_model = model.fit()

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_start = pd.to_datetime('2024-01-07')  # First Sunday of 2024
forecast_end = pd.to_datetime('2024-12-29')  # Last Sunday of 2024
forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
forecast_values = hw_model.forecast(steps=forecast_steps)
forecast_values.index = forecast_index

# Evaluation for 2023
forecast_2023 = hw_model.forecast(steps=len(test))
mae = mean_absolute_error(test, forecast_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Print the first few forecasted values for 2024
forecast_df = pd.DataFrame({'Forecasted Temperature': forecast_values})
print(forecast_df.head(10))

# Save the forecasted values for 2024 to a CSV file
forecast_df.to_csv('forecasted_temperature_2024_hw.csv')

import pandas as pd
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Load the data and check column names
df = pd.read_csv("jordan-airports-historical-observations.csv")
print(df.columns)  # Check if 'time' column exists

# Ensure 'time' column is in datetime format and set as index
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)

# Sort the DataFrame by index to ensure it's in chronological order
df = df.sort_index()

# Resample the data to weekly frequency, taking the mean for each week
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Fit the Exponential Smoothing (Holt-Winters) model
model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=52)
hw_model = model.fit()

# Forecasting for the whole year of 2024
forecast_steps = 52  # Number of weeks in 2024
forecast_start = pd.to_datetime('2024-01-07')  # First Sunday of 2024
forecast_end = pd.to_datetime('2024-12-29')  # Last Sunday of 2024
forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
forecast_values = hw_model.forecast(steps=forecast_steps)
forecast_values.index = forecast_index

# Evaluation for 2023
forecast_2023 = hw_model.forecast(steps=len(test))
mae = mean_absolute_error(test, forecast_2023)
rmse = np.sqrt(mean_squared_error(test, forecast_2023))

# Print evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(train, label='Train')
plt.plot(test, label='Test')
plt.plot(forecast_2023, label='Forecast 2023', color='orange')
plt.plot(forecast_values, label='Forecast 2024', color='green')
plt.title('Weekly Temperature Forecast with Holt-Winters Exponential Smoothing')
plt.xlabel('Date')
plt.ylabel('Temperature')
plt.legend()
plt.show()

# Save the forecasted values for 2024 to a CSV file
forecast_df = pd.DataFrame({'Forecasted Temperature': forecast_values})
forecast_df.to_csv('forecasted_temperature_2024_hw.csv')

!pip install ipywidgets

import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import ipywidgets as widgets
from IPython.display import display

# Load the data
df = pd.read_csv("jordan-airports-historical-observations.csv")
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)
df = df.sort_index()
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Define widget elements
model_selector = widgets.Dropdown(
    options=['SARIMA', 'ARIMA', 'Holt-Winters'],
    value='SARIMA',
    description='Model:',
)
forecast_button = widgets.Button(
    description='Run Forecast',
    button_style='success'
)
output = widgets.Output()

# Define the function to run the selected model and display results
def run_forecast(button):
    with output:
        output.clear_output()
        model_name = model_selector.value

        if model_name == 'SARIMA':
            model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))
            sarima_model = model.fit(disp=False)
            forecast = sarima_model.get_forecast(steps=52)
            forecast_values = forecast.predicted_mean
            forecast_conf_int = forecast.conf_int()

        elif model_name == 'ARIMA':
            model = ARIMA(train, order=(5, 1, 0))
            arima_model = model.fit()
            forecast = arima_model.get_forecast(steps=52)
            forecast_values = forecast.predicted_mean
            forecast_conf_int = forecast.conf_int()

        elif model_name == 'Holt-Winters':
            model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=52)
            hw_model = model.fit()
            forecast_values = hw_model.forecast(steps=52)

        forecast_start = pd.to_datetime('2024-01-07')
        forecast_end = pd.to_datetime('2024-12-29')
        forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
        forecast_values.index = forecast_index
        if model_name in ['SARIMA', 'ARIMA']:
            forecast_conf_int.index = forecast_index

        # Evaluation for 2023
        if model_name == 'SARIMA':
            forecast_2023 = sarima_model.get_forecast(steps=len(test))
        elif model_name == 'ARIMA':
            forecast_2023 = arima_model.get_forecast(steps=len(test))
        elif model_name == 'Holt-Winters':
            forecast_2023 = hw_model.forecast(steps=len(test))

        forecast_values_2023 = forecast_2023.predicted_mean
        mae = mean_absolute_error(test, forecast_values_2023)
        rmse = np.sqrt(mean_squared_error(test, forecast_values_2023))

        # Display evaluation metrics and forecast
        print(f"Model: {model_name}")
        print(f"Mean Absolute Error (MAE): {mae}")
        print(f"Root Mean Squared Error (RMSE): {rmse}")
        print(f"Forecasted values for 2024:")
        display(forecast_values.head(10))

        # Optionally, save to CSV
        forecast_df = pd.DataFrame({'Forecasted Temperature': forecast_values})
        if model_name in ['SARIMA', 'ARIMA']:
            forecast_df['Lower Confidence Interval'] = forecast_conf_int.iloc[:, 0]
            forecast_df['Upper Confidence Interval'] = forecast_conf_int.iloc[:, 1]
        forecast_df.to_csv(f'forecasted_temperature_2024_{model_name.lower()}.csv')

forecast_button.on_click(run_forecast)

# Display the widgets
display(model_selector, forecast_button, output)

import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import ipywidgets as widgets
from IPython.display import display

# Load the data
df = pd.read_csv("jordan-airports-historical-observations.csv")
df['time'] = pd.to_datetime(df['time'])
df.set_index('time', inplace=True)
df = df.sort_index()
data = df['temperature'].resample('W').mean()

# Split the data into train and test sets
split_date = '2023-01-01'
train = data[:split_date]
test = data[split_date:]

# Define the forecast range
forecast_start = pd.to_datetime('2024-01-07')
forecast_end = pd.to_datetime('2024-12-29')

# Define widget elements
model_selector = widgets.Dropdown(
    options=['SARIMA', 'ARIMA', 'Holt-Winters'],
    value='SARIMA',
    description='Model:',
)
date_picker = widgets.DatePicker(
    description='Date:',
    disabled=False,
    min=forecast_start,
    max=forecast_end
)
forecast_button = widgets.Button(
    description='Get Forecast',
    button_style='success'
)
output = widgets.Output()

# Define the function to run the selected model and display results
def run_forecast(button):
    with output:
        output.clear_output()
        model_name = model_selector.value
        selected_date = date_picker.value

        if selected_date is None:
            print("Please select a date.")
            return

        selected_date = pd.Timestamp(selected_date)

        if selected_date < forecast_start or selected_date > forecast_end:
            print("Selected date is out of the forecast range.")
            return

        # Forecasting setup
        forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')

        try:
            if model_name == 'SARIMA':
                model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52), enforce_stationarity=False, enforce_invertibility=False)
                sarima_model = model.fit(disp=False)
                forecast = sarima_model.get_forecast(steps=52)
                forecast_values = forecast.predicted_mean

            elif model_name == 'ARIMA':
                model = ARIMA(train, order=(5, 1, 0))
                arima_model = model.fit()
                forecast = arima_model.get_forecast(steps=52)
                forecast_values = forecast.predicted_mean

            elif model_name == 'Holt-Winters':
                model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=52, initialization_method="estimated")
                hw_model = model.fit()
                forecast_values = hw_model.forecast(steps=52)

            forecast_values.index = forecast_index

            # Map the selected date to the nearest forecasted date
            nearest_forecast_date = min(forecast_index, key=lambda d: abs(d - selected_date))

            # Debugging: print forecast dates and selected date
            print("Forecast range:", forecast_start, "to", forecast_end)
            print("Selected date:", selected_date)
            print("Nearest forecast date:", nearest_forecast_date)
            print("Forecast index range:", forecast_values.index.min(), "to", forecast_values.index.max())

            # Find the forecasted temperature for the nearest forecast date
            if nearest_forecast_date in forecast_values.index:
                temperature_forecast = forecast_values[nearest_forecast_date]
                print(f"Forecasted temperature for {nearest_forecast_date.date()}: {temperature_forecast:.2f}¬∞C")
            else:
                print("Selected date is out of the forecast range.")

        except Exception as e:
            print(f"Error fitting the {model_name} model: {e}")

forecast_button.on_click(run_forecast)

# Display the widgets
display(model_selector, date_picker, forecast_button, output)

pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# from statsmodels.tsa.statespace.sarimax import SARIMAX
# from statsmodels.tsa.arima.model import ARIMA
# from statsmodels.tsa.holtwinters import ExponentialSmoothing
# import datetime
# 
# # Load and prepare data
# df = pd.read_csv("jordan-airports-historical-observations.csv")
# df['time'] = pd.to_datetime(df['time'])
# df.set_index('time', inplace=True)
# df = df.sort_index()
# data = df['temperature'].resample('W').mean()
# 
# # Split data
# split_date = '2023-01-01'
# train = data[:split_date]
# forecast_start = pd.to_datetime('2024-01-07')
# forecast_end = pd.to_datetime('2024-12-29')
# forecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq='W')
# 
# # Streamlit UI
# st.title("Weekly Temperature Forecast")
# model_name = st.selectbox("Select Model", ['SARIMA', 'ARIMA', 'Holt-Winters'])
# date_input = st.date_input("Select a date", min_value=forecast_start, max_value=forecast_end)
# 
# if st.button("Get Forecast"):
#     if model_name == 'SARIMA':
#         model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52),
#                         enforce_stationarity=False, enforce_invertibility=False)
#         model_fit = model.fit(disp=False)
#         forecast = model_fit.get_forecast(steps=52).predicted_mean
#     elif model_name == 'ARIMA':
#         model = ARIMA(train, order=(5, 1, 0))
#         model_fit = model.fit()
#         forecast = model_fit.get_forecast(steps=52).predicted_mean
#     elif model_name == 'Holt-Winters':
#         model = ExponentialSmoothing(train, trend='add', seasonal='add',
#                                      seasonal_periods=52, initialization_method="estimated")
#         model_fit = model.fit()
#         forecast = model_fit.forecast(steps=52)
# 
#     forecast.index = forecast_index
#     selected_date = pd.Timestamp(date_input)
#     nearest_date = min(forecast.index, key=lambda d: abs(d - selected_date))
# 
#     if nearest_date in forecast.index:
#         st.success(f"Forecasted temperature for {nearest_date.date()}: {forecast[nearest_date]:.2f}¬∞C")
#     else:
#         st.error("Selected date is out of forecast range.")
#

!streamlit run app.py &> /dev/null &

!streamlit run app.py &>/content/logs.txt &

!pip install pyngrok

from pyngrok import ngrok
ngrok.set_auth_token("2waUQo4mvTEkpDFPLKrvtNyrzNg_3bS41yDT4HLL4KAcmtvFt")

from pyngrok import ngrok

# Use your authtoken first (only once)
ngrok.set_auth_token("2waUQo4mvTEkpDFPLKrvtNyrzNg_3bS41yDT4HLL4KAcmtvFt")  # Only needed once

public_url = ngrok.connect(8501)
print(f"üåê Streamlit app is live at: {public_url}")